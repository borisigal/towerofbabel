# Story 4.2: Create Outbound Optimization LLM Prompt and API Logic

<!-- Powered by BMADâ„¢ Core -->

## Status

**Draft**

---

## Story

**As a** developer,
**I want** a specialized LLM prompt for outbound optimization,
**so that** the AI provides culturally-appropriate message improvements.

---

## Acceptance Criteria

1. Outbound prompt template created in LLM service layer (`lib/llm/prompts.ts`)
2. Prompt instructs LLM to:
   - Analyze how the message will be perceived in receiver's culture
   - Identify potential misinterpretations or unintended tones
   - Suggest optimized version that's clearer and more culturally appropriate
   - Return structured JSON: `{originalAnalysis: string, suggestions: string[], optimizedMessage: string, emotions: [...same as inbound]}`
3. Prompt maintains "explain like 14-year-old" tone for analysis
4. API route `/api/interpret` handles both inbound and outbound modes based on request parameter
5. Outbound interpretations saved with `interpretation_type="outbound"` in database
6. Usage tracking counts outbound interpretations same as inbound (1 message used)
7. Cost tracking records LLM cost for outbound calls (may differ from inbound)
8. Error handling for malformed outbound responses
9. Unit tests for outbound prompt generation
10. Outbound optimization respects same tier limits as inbound (trial: 10 total messages)

---

## Tasks / Subtasks

### Phase 1: Outbound Prompt Template Design

- [ ] **Task 1: Create Outbound Same-Culture Prompt** (AC: 1, 2, 3)
  - [ ] Add `generateOutboundSameCulturePrompt()` function to `lib/llm/prompts.ts`
  - [ ] Prompt structure:
    - System message: "You are a cultural communication expert helping people optimize messages"
    - Analyze how message will be perceived by receiver (same culture)
    - Identify tone issues, unclear phrasing, potential misinterpretations
    - Suggest 3-5 specific improvements
    - Provide optimized version of message
    - Detect top 3 emotions (dynamic, not preset list)
  - [ ] JSON response format:
    ```typescript
    {
      "originalAnalysis": "How the message will be perceived (2-3 sentences)",
      "suggestions": [
        "Specific improvement 1",
        "Specific improvement 2",
        "Specific improvement 3"
      ],
      "optimizedMessage": "Culturally optimized version of the message",
      "emotions": [
        {
          "name": "Emotion name",
          "senderScore": 7,
          "explanation": "Why this emotion is present"
        }
      ]
    }
    ```
  - [ ] Maintain "explain like 14-year-old" tone for originalAnalysis
  - [ ] Add JSDoc documentation with examples

- [ ] **Task 2: Create Outbound Cross-Culture Prompt** (AC: 1, 2, 3)
  - [ ] Add `generateOutboundCrossCulturePrompt()` function to `lib/llm/prompts.ts`
  - [ ] Prompt structure:
    - System message: "You are a cultural communication expert helping people optimize cross-cultural messages"
    - Analyze how message will be perceived by receiver (different culture)
    - Identify cultural mismatches, communication style differences
    - Suggest 3-5 culturally-specific improvements
    - Provide optimized version that resonates with receiver's culture
    - Detect top 3 emotions with dual scoring (sender/receiver intensity)
  - [ ] JSON response format: Same as Task 1, but with `receiverScore` for emotions
  - [ ] Emphasize cultural differences and how to bridge them
  - [ ] Add JSDoc documentation with examples

- [ ] **Task 3: Create Outbound Prompt Router** (AC: 4)
  - [ ] Add `generateOutboundOptimizationPrompt()` function to `lib/llm/prompts.ts`
  - [ ] Route to same-culture or cross-culture outbound template
  - [ ] Signature: `generateOutboundOptimizationPrompt(message, senderCulture, receiverCulture, sameCulture): string`
  - [ ] Add JSDoc documentation

### Phase 2: LLM Adapter Updates

- [ ] **Task 4: Update LLM Response Type for Outbound** (AC: 2)
  - [ ] Review `lib/llm/types.ts` for response types
  - [ ] Add `OutboundInterpretationResponse` interface:
    ```typescript
    export interface OutboundInterpretationResponse {
      originalAnalysis: string;
      suggestions: string[];
      optimizedMessage: string;
      emotions: Emotion[];
    }
    ```
  - [ ] Keep existing `InboundInterpretationResponse` unchanged
  - [ ] Add union type: `InterpretationResponse = InboundInterpretationResponse | OutboundInterpretationResponse`
  - [ ] Update LLMProvider interface `interpret()` method to return `InterpretationResponse`

- [ ] **Task 5: Update Anthropic Adapter for Outbound Mode** (AC: 4)
  - [ ] Modify `lib/llm/anthropicAdapter.ts` `interpret()` method
  - [ ] Accept mode parameter: `interpret(message, senderCulture, receiverCulture, mode, sameCulture)`
  - [ ] Route to correct prompt generator based on mode:
    ```typescript
    const prompt = mode === 'inbound'
      ? generateInterpretationPrompt(message, senderCulture, receiverCulture, sameCulture)
      : generateOutboundOptimizationPrompt(message, senderCulture, receiverCulture, sameCulture);
    ```
  - [ ] Parse LLM response according to mode (inbound vs outbound schema)
  - [ ] Validate outbound response has required fields (originalAnalysis, suggestions, optimizedMessage, emotions)
  - [ ] Throw `LLMParsingError` if response format is invalid

### Phase 3: API Route Updates

- [ ] **Task 6: Update /api/interpret to Route Based on Mode** (AC: 4, 5)
  - [ ] Modify `app/api/interpret/route.ts` POST handler
  - [ ] Pass mode parameter to LLM adapter:
    ```typescript
    const result = await llmProvider.interpret(
      validatedData.message,
      validatedData.sender_culture as CultureCode,
      validatedData.receiver_culture as CultureCode,
      validatedData.mode as 'inbound' | 'outbound',
      sameCulture
    );
    ```
  - [ ] Update interpretation persistence to include correct interpretation_type:
    ```typescript
    await createInterpretation({
      user_id: user.id,
      culture_sender: validatedData.sender_culture,
      culture_receiver: validatedData.receiver_culture,
      interpretation_type: validatedData.mode, // 'inbound' or 'outbound'
      // ... other fields
    });
    ```
  - [ ] Verify validation already accepts mode (it does - line 140-145)

- [ ] **Task 7: Update API Response Format for Outbound** (AC: 2)
  - [ ] API returns different result structure based on mode
  - [ ] Inbound response (unchanged):
    ```typescript
    {
      success: true,
      data: {
        interpretation: {
          bottomLine: string,
          culturalContext: string,
          emotions: Emotion[]
        }
      },
      metadata: { messages_remaining: number }
    }
    ```
  - [ ] Outbound response:
    ```typescript
    {
      success: true,
      data: {
        interpretation: {
          originalAnalysis: string,
          suggestions: string[],
          optimizedMessage: string,
          emotions: Emotion[]
        }
      },
      metadata: { messages_remaining: number }
    }
    ```
  - [ ] Frontend (Story 4.3) will handle different response structures

### Phase 4: Usage and Cost Tracking

- [ ] **Task 8: Verify Usage Tracking for Outbound** (AC: 6)
  - [ ] Review `incrementUserUsage()` call in API route
  - [ ] Confirm it increments messages_used_count regardless of mode
  - [ ] Verify PAYG usage reporting to Lemon Squeezy includes outbound interpretations
  - [ ] Test that trial users consume 1 of 10 messages for outbound (same as inbound)
  - [ ] No code changes needed (usage tracking is mode-agnostic)

- [ ] **Task 9: Verify Cost Tracking for Outbound** (AC: 7)
  - [ ] Review `trackCost()` call in API route
  - [ ] Confirm it tracks LLM cost from result.metadata.costUsd
  - [ ] Verify cost circuit breaker applies to outbound calls
  - [ ] Log outbound costs separately for analytics:
    ```typescript
    log.info('Interpretation successful', {
      mode: validatedData.mode,
      cost_usd: result.metadata.costUsd,
      // ... other fields
    });
    ```
  - [ ] No code changes needed (cost tracking is mode-agnostic)

### Phase 5: Error Handling

- [ ] **Task 10: Add Error Handling for Malformed Outbound Responses** (AC: 8)
  - [ ] Update `parseInterpretationResponse()` in `lib/llm/anthropicAdapter.ts`
  - [ ] Validate outbound response schema:
    ```typescript
    if (mode === 'outbound') {
      if (!response.originalAnalysis || typeof response.originalAnalysis !== 'string') {
        throw new LLMParsingError('Missing or invalid originalAnalysis field');
      }
      if (!Array.isArray(response.suggestions) || response.suggestions.length === 0) {
        throw new LLMParsingError('Missing or invalid suggestions array');
      }
      if (!response.optimizedMessage || typeof response.optimizedMessage !== 'string') {
        throw new LLMParsingError('Missing or invalid optimizedMessage field');
      }
      if (!Array.isArray(response.emotions) || response.emotions.length !== 3) {
        throw new LLMParsingError('Must include exactly 3 emotions');
      }
    }
    ```
  - [ ] API route already catches `LLMParsingError` and returns 500 with error message
  - [ ] Add structured logging for outbound parsing errors

### Phase 6: Testing

- [ ] **Task 11: Write Unit Tests for Outbound Prompts** (AC: 9)
  - [ ] Create `tests/unit/lib/llm/prompts-outbound.test.ts`
  - [ ] Test: `generateOutboundSameCulturePrompt()` includes all required sections
  - [ ] Test: Outbound same-culture prompt mentions "optimize" and "improve"
  - [ ] Test: Outbound cross-culture prompt emphasizes cultural differences
  - [ ] Test: `generateOutboundOptimizationPrompt()` routes correctly based on sameCulture flag
  - [ ] Test: Prompts include JSON format instructions
  - [ ] Test: Prompts specify "explain like 14-year-old" tone
  - [ ] Use Vitest

- [ ] **Task 12: Write Unit Tests for Outbound LLM Parsing**
  - [ ] Create `tests/unit/lib/llm/anthropicAdapter-outbound.test.ts`
  - [ ] Test: Valid outbound response parsed correctly
  - [ ] Test: Missing originalAnalysis throws LLMParsingError
  - [ ] Test: Missing suggestions array throws LLMParsingError
  - [ ] Test: Empty suggestions array throws LLMParsingError
  - [ ] Test: Missing optimizedMessage throws LLMParsingError
  - [ ] Test: Missing emotions throws LLMParsingError
  - [ ] Test: Emotions with wrong count throws LLMParsingError
  - [ ] Mock Anthropic API responses
  - [ ] Use Vitest

- [ ] **Task 13: Write Integration Tests for Outbound API Flow**
  - [ ] Create `tests/integration/api/interpret-outbound.test.ts`
  - [ ] Test: POST /api/interpret with mode='outbound' returns optimized message
  - [ ] Test: Outbound interpretation saved with interpretation_type='outbound'
  - [ ] Test: Outbound interpretation increments messages_used_count
  - [ ] Test: Outbound interpretation tracked in cost circuit breaker
  - [ ] Test: Trial user exhausts limit with mix of inbound/outbound interpretations
  - [ ] Test: Invalid mode parameter returns 400 error
  - [ ] Test: Malformed outbound LLM response returns 500 error
  - [ ] Use Vitest + Supertest
  - [ ] Mock Anthropic API

- [ ] **Task 14: Manual Testing Scenarios**
  - [ ] Test: Submit outbound interpretation â†’ API returns originalAnalysis, suggestions, optimizedMessage
  - [ ] Test: Verify outbound response JSON structure matches AC #2
  - [ ] Test: Check database â†’ interpretation_type='outbound'
  - [ ] Test: Trial user at 9/10 messages â†’ submit outbound â†’ limit reached
  - [ ] Test: Check logs â†’ outbound cost tracked correctly
  - [ ] Test: Malformed LLM response â†’ 500 error with helpful message
  - [ ] Test: Same-culture outbound optimization
  - [ ] Test: Cross-culture outbound optimization
  - [ ] Use Lemon Squeezy test mode for PAYG usage reporting

### Phase 7: Build Validation

- [ ] **Task 15: TypeScript Compilation Check**
  - [ ] Run `npx tsc --noEmit`
  - [ ] Fix any TypeScript errors
  - [ ] Ensure strict mode compliance

- [ ] **Task 16: Linting and Formatting**
  - [ ] Run `npm run lint`
  - [ ] Fix ESLint errors (warnings acceptable)
  - [ ] Run `prettier --write` on modified files

- [ ] **Task 17: Test Suite Validation**
  - [ ] Run unit tests: `npm test tests/unit`
  - [ ] Run integration tests: `npm test tests/integration`
  - [ ] Ensure all Story 4.2 tests pass

---

## Dev Notes

### Story Context

**This story enables the backend logic for outbound message optimization.**

**Epic 4 Integration Flow:**
- Story 4.1: Mode toggle UI (DONE)
- **Story 4.2 (THIS STORY):** Outbound LLM prompt and API logic
- Story 4.3: Side-by-side comparison UI for outbound results
- Story 4.4: Thumbs up/down feedback
- Story 4.5: Feedback analytics dashboard

**What Story 4.2 Adds:**
- âœ¨ **NEW:** Outbound optimization prompt templates (same-culture and cross-culture)
- âœ¨ **NEW:** Outbound response type with originalAnalysis, suggestions, optimizedMessage
- ðŸ”§ **MODIFIED:** LLM adapter to route based on mode
- ðŸ”§ **MODIFIED:** API route to handle both inbound and outbound modes
- ðŸ”§ **MODIFIED:** Database saves interpretation_type correctly

**Key Design Decisions:**
- Separate prompt templates for outbound (not a modification of inbound prompts)
- Outbound prompts focus on optimization and improvement (not just analysis)
- Same usage and cost tracking for both modes (simplifies billing logic)
- Tier limits apply equally to inbound and outbound (no separate quotas)

---

### Technical Implementation Details

#### Outbound Prompt Template Design

**File: `lib/llm/prompts.ts`**

```typescript
/**
 * Generates prompt for outbound optimization (same culture).
 * Focuses on improving message clarity and tone for receiver in same culture.
 *
 * @param message - The message user wants to send
 * @param culture - Shared culture of sender and receiver
 * @returns Formatted prompt for outbound optimization
 *
 * @example
 * ```typescript
 * const prompt = generateOutboundSameCulturePrompt(
 *   'Can you finish this by tomorrow?',
 *   'american'
 * );
 * // Prompt asks LLM to suggest more polite phrasing
 * ```
 */
export function generateOutboundSameCulturePrompt(
  message: string,
  culture: CultureCode
): string {
  const cultureName = CULTURE_NAMES[culture];

  return `${SYSTEM_MESSAGE}

You are helping someone in ${cultureName} culture optimize a message they want to send to another ${cultureName} person.

Analyze the message and provide:
1. **Original Analysis**: Explain how the message will likely be perceived by the receiver (2-3 sentences, explain like you're talking to a 14-year-old)
2. **Suggestions**: List 3-5 specific ways to improve the message (be concrete, not vague)
3. **Optimized Message**: Provide a culturally optimized version that's clearer, more appropriate, and better received
4. **Emotions**: Detect the top 3 emotions present in the ORIGINAL message (detect these dynamically, don't use a preset list)

Message to optimize:
"""
${message}
"""

Provide your analysis in the following JSON format (return ONLY the JSON, no other text):
{
  "originalAnalysis": "How the receiver will likely perceive this message (2-3 sentences)",
  "suggestions": [
    "Specific improvement 1: [concrete suggestion]",
    "Specific improvement 2: [concrete suggestion]",
    "Specific improvement 3: [concrete suggestion]"
  ],
  "optimizedMessage": "The improved version of the message that will be better received",
  "emotions": [
    {
      "name": "Emotion name (detect dynamically)",
      "senderScore": 7,
      "explanation": "Brief explanation of why this emotion is present in the original message"
    }
  ]
}

IMPORTANT:
- Return ONLY the JSON object, no markdown formatting, no code blocks, no additional text
- Include 3-5 suggestions (minimum 3, maximum 5)
- Suggestions must be specific and actionable (not vague like "be nicer")
- optimizedMessage should feel natural, not robotic or overly formal
- Include exactly 3 emotions (top 3 most relevant in the ORIGINAL message)
- Emotion scores must be integers between 0-10
- For same culture, only include senderScore (not receiverScore)`;
}

/**
 * Generates prompt for outbound optimization (cross-culture).
 * Focuses on bridging cultural differences and avoiding misunderstandings.
 *
 * @param message - The message user wants to send
 * @param senderCulture - Culture code of message sender
 * @param receiverCulture - Culture code of message receiver
 * @returns Formatted prompt for cross-culture outbound optimization
 *
 * @example
 * ```typescript
 * const prompt = generateOutboundCrossCulturePrompt(
 *   'I appreciate your hard work on this project.',
 *   'american',
 *   'japanese'
 * );
 * // Prompt suggests more indirect phrasing for Japanese receiver
 * ```
 */
export function generateOutboundCrossCulturePrompt(
  message: string,
  senderCulture: CultureCode,
  receiverCulture: CultureCode
): string {
  const senderCultureName = CULTURE_NAMES[senderCulture];
  const receiverCultureName = CULTURE_NAMES[receiverCulture];

  return `${SYSTEM_MESSAGE}

You are helping someone in ${senderCultureName} culture optimize a message they want to send to someone in ${receiverCultureName} culture.

This is a CROSS-CULTURE message. Focus on:
- How the message might be misunderstood due to cultural differences
- What communication style differences exist between these cultures
- How to bridge the cultural gap and make the message resonate with the receiver

Provide:
1. **Original Analysis**: Explain how the ${receiverCultureName} receiver will likely perceive this ${senderCultureName}-style message (3-4 sentences, explain like you're talking to a 14-year-old)
2. **Suggestions**: List 3-5 culturally-specific improvements that bridge the gap between ${senderCultureName} and ${receiverCultureName} communication styles
3. **Optimized Message**: Provide a culturally optimized version that resonates with ${receiverCultureName} culture while preserving the sender's intent
4. **Emotions**: Detect the top 3 emotions with BOTH sender and receiver intensity scores (how intense in each culture)

Message to optimize:
"""
${message}
"""

Provide your analysis in the following JSON format (return ONLY the JSON, no other text):
{
  "originalAnalysis": "How this message will likely be perceived by the ${receiverCultureName} receiver, including potential cultural misunderstandings (3-4 sentences)",
  "suggestions": [
    "Cultural improvement 1: [specific to ${receiverCultureName} culture]",
    "Cultural improvement 2: [bridges gap between cultures]",
    "Cultural improvement 3: [adapts communication style]"
  ],
  "optimizedMessage": "The culturally adapted version that will resonate with ${receiverCultureName} receiver",
  "emotions": [
    {
      "name": "Emotion name (detect dynamically)",
      "senderScore": 7,
      "receiverScore": 3,
      "explanation": "Brief explanation of how this emotion is expressed/perceived differently across these cultures"
    }
  ]
}

IMPORTANT:
- Return ONLY the JSON object, no markdown formatting, no code blocks, no additional text
- Include 3-5 suggestions (minimum 3, maximum 5)
- Suggestions must be culturally specific (not generic advice)
- optimizedMessage should adapt to ${receiverCultureName} communication norms
- Include exactly 3 emotions (top 3 most relevant in the ORIGINAL message)
- Emotion scores must be integers between 0-10
- For cross-culture, include BOTH senderScore and receiverScore
- Explain how emotions may be expressed or perceived differently across these cultures`;
}

/**
 * Generates the appropriate outbound prompt based on cultural context.
 * Routes to same-culture or cross-culture outbound template.
 *
 * @param message - The message user wants to send
 * @param senderCulture - Culture code of message sender
 * @param receiverCulture - Culture code of message receiver
 * @param sameCulture - Whether sender and receiver share the same culture
 * @returns Formatted prompt for outbound optimization
 */
export function generateOutboundOptimizationPrompt(
  message: string,
  senderCulture: CultureCode,
  receiverCulture: CultureCode,
  sameCulture: boolean
): string {
  if (sameCulture) {
    return generateOutboundSameCulturePrompt(message, senderCulture);
  } else {
    return generateOutboundCrossCulturePrompt(message, senderCulture, receiverCulture);
  }
}
```

**Prompt Design Rationale:**
- **Clear Structure**: originalAnalysis â†’ suggestions â†’ optimizedMessage â†’ emotions
- **Actionable Suggestions**: "Specific improvement" not "be nicer" (concrete, not vague)
- **Natural Tone**: optimizedMessage should sound human, not robotic
- **Cultural Adaptation**: Cross-culture prompts emphasize bridging communication gaps
- **14-Year-Old Tone**: originalAnalysis maintains accessibility standard from inbound

[Source: Epic 4 Story 4.2 AC #2, #3]

---

#### Updated LLM Adapter with Mode Routing

**File: `lib/llm/anthropicAdapter.ts`**

```typescript
import {
  generateInterpretationPrompt,
  generateOutboundOptimizationPrompt
} from './prompts';

/**
 * Anthropic (Claude) LLM adapter implementation.
 * Handles cultural interpretation and outbound optimization via Claude API.
 */
export class AnthropicAdapter implements LLMProvider {
  // ... existing constructor and methods

  /**
   * Interprets message or optimizes outbound message via Claude API.
   *
   * CRITICAL: Always queries database for tier/usage before calling this method.
   * Cost tracking occurs immediately after this call returns.
   *
   * @param message - Message text to interpret or optimize
   * @param senderCulture - Culture code of message sender
   * @param receiverCulture - Culture code of message receiver
   * @param mode - Interpretation mode: 'inbound' or 'outbound'
   * @param sameCulture - Whether sender and receiver share the same culture
   * @returns Interpretation or optimization result with metadata
   *
   * @throws {LLMTimeoutError} If API call exceeds timeout
   * @throws {LLMRateLimitError} If API rate limit exceeded
   * @throws {LLMAuthError} If API key invalid
   * @throws {LLMParsingError} If response format is invalid
   * @throws {LLMProviderError} For other API errors
   */
  async interpret(
    message: string,
    senderCulture: CultureCode,
    receiverCulture: CultureCode,
    mode: 'inbound' | 'outbound',
    sameCulture: boolean
  ): Promise<InterpretationResult> {
    const startTime = Date.now();

    // Generate appropriate prompt based on mode
    const prompt = mode === 'inbound'
      ? generateInterpretationPrompt(message, senderCulture, receiverCulture, sameCulture)
      : generateOutboundOptimizationPrompt(message, senderCulture, receiverCulture, sameCulture);

    try {
      log.info('Calling Anthropic API', {
        mode,
        senderCulture,
        receiverCulture,
        sameCulture,
        messageLength: message.length
      });

      const response = await this.client.messages.create({
        model: this.modelName,
        max_tokens: 1500,
        temperature: 0.7,
        messages: [
          {
            role: 'user',
            content: prompt
          }
        ]
      });

      // Parse response based on mode
      const parsedResult = this.parseInterpretationResponse(
        response,
        mode,
        sameCulture
      );

      const responseTimeMs = Date.now() - startTime;

      log.info('Anthropic API call successful', {
        mode,
        responseTimeMs,
        tokensInput: response.usage.input_tokens,
        tokensOutput: response.usage.output_tokens
      });

      return {
        interpretation: parsedResult,
        metadata: {
          costUsd: this.calculateCost(response.usage),
          responseTimeMs,
          tokensInput: response.usage.input_tokens,
          tokensOutput: response.usage.output_tokens,
          llmProvider: 'anthropic',
          llmModel: this.modelName
        }
      };

    } catch (error) {
      // ... existing error handling
    }
  }

  /**
   * Parses LLM response into structured interpretation or optimization result.
   *
   * @param response - Raw Anthropic API response
   * @param mode - Interpretation mode: 'inbound' or 'outbound'
   * @param sameCulture - Whether sender and receiver share the same culture
   * @returns Parsed interpretation or optimization result
   *
   * @throws {LLMParsingError} If response format is invalid
   */
  private parseInterpretationResponse(
    response: any,
    mode: 'inbound' | 'outbound',
    sameCulture: boolean
  ): InterpretationResponse {
    // Extract text content from Claude response
    const content = response.content[0]?.text;
    if (!content) {
      throw new LLMParsingError('No content in LLM response');
    }

    // Parse JSON response
    let parsed: any;
    try {
      // Remove markdown code blocks if present (Claude sometimes adds them)
      const cleanContent = content.replace(/```json\n?/g, '').replace(/```\n?/g, '');
      parsed = JSON.parse(cleanContent);
    } catch (error) {
      throw new LLMParsingError(`Invalid JSON in LLM response: ${error.message}`);
    }

    // Validate response based on mode
    if (mode === 'inbound') {
      return this.validateInboundResponse(parsed, sameCulture);
    } else {
      return this.validateOutboundResponse(parsed, sameCulture);
    }
  }

  /**
   * Validates inbound interpretation response structure.
   */
  private validateInboundResponse(parsed: any, sameCulture: boolean): InboundInterpretationResponse {
    // ... existing inbound validation logic
  }

  /**
   * Validates outbound optimization response structure.
   *
   * @param parsed - Parsed JSON response
   * @param sameCulture - Whether sender and receiver share the same culture
   * @returns Validated outbound interpretation response
   *
   * @throws {LLMParsingError} If response format is invalid
   */
  private validateOutboundResponse(parsed: any, sameCulture: boolean): OutboundInterpretationResponse {
    // Validate originalAnalysis
    if (!parsed.originalAnalysis || typeof parsed.originalAnalysis !== 'string') {
      throw new LLMParsingError('Missing or invalid originalAnalysis field in outbound response');
    }

    // Validate suggestions array
    if (!Array.isArray(parsed.suggestions)) {
      throw new LLMParsingError('Missing or invalid suggestions array in outbound response');
    }

    if (parsed.suggestions.length < 3 || parsed.suggestions.length > 5) {
      throw new LLMParsingError('Suggestions array must contain 3-5 items');
    }

    for (const suggestion of parsed.suggestions) {
      if (typeof suggestion !== 'string' || suggestion.length === 0) {
        throw new LLMParsingError('Each suggestion must be a non-empty string');
      }
    }

    // Validate optimizedMessage
    if (!parsed.optimizedMessage || typeof parsed.optimizedMessage !== 'string') {
      throw new LLMParsingError('Missing or invalid optimizedMessage field in outbound response');
    }

    // Validate emotions array
    if (!Array.isArray(parsed.emotions) || parsed.emotions.length !== 3) {
      throw new LLMParsingError('Must include exactly 3 emotions in outbound response');
    }

    const validatedEmotions: Emotion[] = [];

    for (const emotion of parsed.emotions) {
      // Validate emotion structure
      if (!emotion.name || typeof emotion.name !== 'string') {
        throw new LLMParsingError('Each emotion must have a valid name');
      }

      if (!Number.isInteger(emotion.senderScore) || emotion.senderScore < 0 || emotion.senderScore > 10) {
        throw new LLMParsingError('Emotion senderScore must be an integer between 0-10');
      }

      // Cross-culture requires receiverScore
      if (!sameCulture) {
        if (!Number.isInteger(emotion.receiverScore) || emotion.receiverScore < 0 || emotion.receiverScore > 10) {
          throw new LLMParsingError('Cross-culture emotion must have receiverScore between 0-10');
        }
      }

      if (!emotion.explanation || typeof emotion.explanation !== 'string') {
        throw new LLMParsingError('Each emotion must have a valid explanation');
      }

      validatedEmotions.push({
        name: emotion.name,
        senderScore: emotion.senderScore,
        receiverScore: sameCulture ? undefined : emotion.receiverScore,
        explanation: emotion.explanation
      });
    }

    return {
      originalAnalysis: parsed.originalAnalysis,
      suggestions: parsed.suggestions,
      optimizedMessage: parsed.optimizedMessage,
      emotions: validatedEmotions
    };
  }
}
```

[Source: Epic 4 Story 4.2 AC #4, #8]

---

#### API Route Mode Handling

**File: `app/api/interpret/route.ts`**

```typescript
export async function POST(req: NextRequest) {
  // ... existing authentication, rate limiting, validation

  // Extract validated data
  const { message, sender_culture, receiver_culture, mode } = validatedData;

  // ... existing authorization, usage limit check, cost circuit breaker

  // 7. BUSINESS LOGIC - Call LLM with mode parameter
  const sameCulture = sender_culture === receiver_culture;
  const llmProvider = createLLMProvider();
  const startTime = Date.now();

  try {
    const result = await llmProvider.interpret(
      message,
      sender_culture as CultureCode,
      receiver_culture as CultureCode,
      mode as 'inbound' | 'outbound', // Pass mode to LLM adapter
      sameCulture
    );

    // 8. COST TRACKING - CRITICAL
    await trackCost(user.id, result.metadata.costUsd);

    // 9. PERSISTENCE - Save with correct interpretation_type
    await createInterpretation({
      user_id: user.id,
      culture_sender: sender_culture,
      culture_receiver: receiver_culture,
      character_count: message.length,
      interpretation_type: mode, // 'inbound' or 'outbound'
      cost_usd: result.metadata.costUsd,
      llm_provider: result.metadata.llmProvider,
      response_time_ms: result.metadata.responseTimeMs,
      tokens_input: result.metadata.tokensInput,
      tokens_output: result.metadata.tokensOutput,
      tokens_cached: result.metadata.tokensCached
    });

    // Increment usage counter (mode-agnostic)
    await incrementUserUsage(user.id);

    // Report usage to Lemon Squeezy for PAYG users (mode-agnostic)
    if (userRecord.tier === 'payg') {
      await reportInterpretationUsage(user.id, 1);
    }

    // 10. LOGGING (structured)
    log.info('Interpretation successful', {
      user_id: user.id,
      mode: mode,
      culture_pair: `${sender_culture}-${receiver_culture}`,
      cost_usd: result.metadata.costUsd,
      response_time_ms: result.metadata.responseTimeMs,
    });

    // 11. RESPONSE
    return NextResponse.json({
      success: true,
      data: {
        interpretation: result.interpretation // Structure differs by mode
      },
      metadata: {
        messages_remaining: usageCheck.messagesRemaining ? usageCheck.messagesRemaining - 1 : undefined
      }
    });

  } catch (error) {
    // ... existing error handling (LLMParsingError already caught)
  }
}
```

**Key Changes:**
- Pass `mode` parameter to `llmProvider.interpret()`
- Save `interpretation_type: mode` in database
- Usage and cost tracking remain mode-agnostic (no special handling needed)
- Response structure differs based on mode (frontend handles in Story 4.3)

[Source: Epic 4 Story 4.2 AC #4, #5, #6, #7]

---

### File Locations and Project Structure

**Files to Create:**
```
/tests/unit/lib/llm/
  â”œâ”€â”€ prompts-outbound.test.ts            # CREATE: Outbound prompt unit tests
  â””â”€â”€ anthropicAdapter-outbound.test.ts   # CREATE: Outbound parsing unit tests

/tests/integration/api/
  â””â”€â”€ interpret-outbound.test.ts          # CREATE: Outbound API integration tests
```

**Files to Modify:**
```
/lib/llm/
  â”œâ”€â”€ prompts.ts                          # MODIFY: Add outbound prompt functions
  â”œâ”€â”€ types.ts                            # MODIFY: Add OutboundInterpretationResponse
  â””â”€â”€ anthropicAdapter.ts                 # MODIFY: Add mode parameter, validate outbound responses

/app/api/interpret/
  â””â”€â”€ route.ts                            # MODIFY: Pass mode to LLM, save interpretation_type
```

**Database Schema:**
```
/prisma/schema.prisma
  â””â”€â”€ Interpretation.interpretation_type  # EXISTING: Already supports "inbound" | "outbound"
```

[Source: architecture/12-unified-project-structure.md]

---

### Relevant Source Tree

```
towerofbabel/
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ llm/
â”‚       â”œâ”€â”€ prompts.ts                    # MODIFY: Add outbound prompts
â”‚       â”œâ”€â”€ types.ts                      # MODIFY: Add OutboundInterpretationResponse
â”‚       â””â”€â”€ anthropicAdapter.ts           # MODIFY: Handle mode routing
â”œâ”€â”€ app/
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ interpret/
â”‚           â””â”€â”€ route.ts                  # MODIFY: Pass mode, save interpretation_type
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â”‚       â””â”€â”€ llm/
â”‚   â”‚           â”œâ”€â”€ prompts-outbound.test.ts  # CREATE
â”‚   â”‚           â””â”€â”€ anthropicAdapter-outbound.test.ts  # CREATE
â”‚   â””â”€â”€ integration/
â”‚       â””â”€â”€ api/
â”‚           â””â”€â”€ interpret-outbound.test.ts     # CREATE
â””â”€â”€ prisma/
    â””â”€â”€ schema.prisma                      # EXISTING: interpretation_type field
```

---

### Testing

#### Testing Strategy

**Unit Tests (Target: 80% Coverage):**

1. **Outbound Prompt Generation Tests** (`prompts-outbound.test.ts`):
   - `generateOutboundSameCulturePrompt()` includes all required sections
   - Outbound same-culture prompt mentions "optimize" and "improve"
   - Outbound cross-culture prompt emphasizes cultural differences
   - `generateOutboundOptimizationPrompt()` routes correctly based on sameCulture flag
   - Prompts include JSON format instructions
   - Prompts specify "explain like 14-year-old" tone

2. **Outbound LLM Parsing Tests** (`anthropicAdapter-outbound.test.ts`):
   - Valid outbound response parsed correctly
   - Missing originalAnalysis throws LLMParsingError
   - Missing suggestions array throws LLMParsingError
   - Empty suggestions array throws LLMParsingError
   - Too few suggestions (< 3) throws LLMParsingError
   - Too many suggestions (> 5) throws LLMParsingError
   - Missing optimizedMessage throws LLMParsingError
   - Missing emotions throws LLMParsingError
   - Emotions with wrong count throws LLMParsingError
   - Cross-culture emotions missing receiverScore throws LLMParsingError

**Integration Tests (Target: 60% Coverage):**

3. **Outbound API Flow Tests** (`interpret-outbound.test.ts`):
   - POST /api/interpret with mode='outbound' returns optimized message
   - Outbound interpretation saved with interpretation_type='outbound'
   - Outbound interpretation increments messages_used_count
   - Outbound interpretation tracked in cost circuit breaker
   - Trial user exhausts limit with mix of inbound/outbound interpretations (5 inbound + 5 outbound = 10 total)
   - Invalid mode parameter returns 400 error
   - Malformed outbound LLM response returns 500 error with LLMParsingError message

**Manual Testing Scenarios:**

4. **Outbound Optimization Tests**:
   - Submit outbound interpretation â†’ API returns originalAnalysis, suggestions, optimizedMessage
   - Verify outbound response JSON structure matches AC #2
   - Check database â†’ interpretation_type='outbound'
   - Trial user at 9/10 messages â†’ submit outbound â†’ limit reached (same as inbound)
   - Check logs â†’ outbound cost tracked correctly
   - Malformed LLM response â†’ 500 error with helpful message
   - Same-culture outbound optimization (American â†’ American)
   - Cross-culture outbound optimization (American â†’ Japanese)
   - PAYG user â†’ outbound interpretation reported to Lemon Squeezy

5. **Mixed Mode Tests**:
   - Trial user: 5 inbound + 5 outbound = 10 total (limit reached)
   - Pro user: Unlimited inbound and outbound interpretations
   - PAYG user: Both modes billed at $0.50 per interpretation

**Testing Framework:**
- **Unit Tests:** Vitest
- **Integration Tests:** Vitest + Supertest
- **Mocking:** Mock Anthropic API responses, use test database

[Source: architecture/16-coding-standards.md#testing-standards]

---

### Prompt Engineering Best Practices

**Why Separate Outbound Prompts:**
- Inbound: Analyze and explain (passive understanding)
- Outbound: Optimize and improve (active refinement)
- Different cognitive tasks require different instructions
- Clear separation prevents prompt confusion

**Outbound Prompt Key Elements:**
1. **originalAnalysis**: How receiver will perceive (empathy)
2. **suggestions**: Concrete improvements (actionable)
3. **optimizedMessage**: Better version (practical output)
4. **emotions**: Original message emotions (context preservation)

**14-Year-Old Tone Rationale:**
- Accessibility (matches inbound interpretation tone)
- Avoids jargon and overcomplicated language
- Users understand why changes are suggested
- Maintains TowerOfBabel's approachable brand voice

[Source: Epic 4 Story 4.2 AC #3, PRD accessibility requirements]

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-30 | 1.0 | Story created with outbound optimization LLM prompt and API logic | Scrum Master (Bob) |

---

## Dev Agent Record

### Agent Model Used

**Model:** [To be filled by dev agent]
**Dev Agent:** [To be filled by dev agent]
**Date:** [To be filled by dev agent]

### Debug Log References

[To be filled by dev agent]

### Completion Notes

[To be filled by dev agent]

### File List

**Created Files:**
[To be filled by dev agent]

**Modified Files:**
[To be filled by dev agent]

---

## QA Results

[To be filled by QA agent]

---
